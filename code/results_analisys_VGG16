#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Sep  8 23:49:44 2018

@author: Carolina
"""

#######reading locally results afte aws calculations
with open('VGG16sum.pkl', 'rb') as handle:
    VGG16sum= pickle.load(handle)
    
with open('ResNet50sum50drp.pkl', 'rb') as handle:
    ResNet50sum50drp= pickle.load(handle)

with open('y_pred_test.pkl', 'rb') as handle:
    y_pred_test= pickle.load(handle)

#plotting ROC curves
plt.figure(figsize= [10,10])
# Plotting our Baseline..
plt.plot([0,1],[0,1])
#plotting model results
plt.plot(VGG16sum['fpr'],VGG16sum['tpr'],label='VGG16', color='royalblue', linewidth=5.0)
plt.plot(ResNet50sum50drp['fpr'],ResNet50sum50drp['tpr'],label='ResNet50', color= 'grey', linewidth=5.0)

#formattinf labels
plt.xlabel('FPR', size = 30)
plt.ylabel('TPR', size = 30,rotation = 0,labelpad = 35)
plt.legend(loc='best',prop={'size': 30})
plt.title("ROC curves for main models", size = 40)
plt.xticks(size=25)
plt.yticks(size=25)
ax = plt.gca()
ax.yaxis.set_label_coords(-0.06,0.97)
ttl = ax.title
ttl.set_position([.5, 1.05]);

y_pred_test=pd.DataFrame(y_pred_test)

recall_score_VGG16 =[]
precision_score_VGG16=[]
f1_score_VGG16=[]

# estimating model metrics
for i in np.linspace(0,1,100):
    y_score =y_pred_test[0] >i
    precision_score_VGG16.append(metrics.precision_score(y_test_vgg16, y_score))#, labels=None, pos_label=1, average=’binary’, sample_weight=None)
    recall_score_VGG16.append(metrics.recall_score(y_test_vgg16, y_score))
    f1_score_VGG16.append(metrics.f1_score(y_test_vgg16, y_score))

# plotting metrics  vs. threshold
plt.figure(figsize= [10,10])
x=np.linspace(0,1,100)
y=recall_score_VGG16
y2=precision_score_VGG16
y3=f1_score_VGG16

ttl = ax.title
ttl.set_position([.5, 1.05])
plt.plot(x,y,label='VGG16_recall',color='gold')
plt.plot(x,y2,label='VGG16_precision',color='royalblue')
plt.plot(x,y3,label='VGG16_f1_score',color='tomato')
#plt.axvline(0.215, color='gold', linestyle='solid')
plt.title("Metrics vs. Threshold  \n", size = 25)
plt.xlabel('Threshold', size = 20, labelpad = 15)
plt.ylabel('Metrics \n ', size = 20, rotation = 0, labelpad = 35)
plt.xticks(size=20)
plt.yticks(size=20)
ax = plt.gca()
ax.yaxis.set_label_coords(-0.06,0.97)
plt.legend(loc='best',prop={'size': 20});

# estimating therhold that would maximize the F1 score
ind=np.argmax(f1_score_VGG16)
tsh=list(np.linspace(0,1,100))[ind]

# estimating F1_score for estimated threshold
y_score =y_pred_test[0] >tsh

f1VGG16=metrics.f1_score(y_test_vgg16, y_score)

# estimatting confusion matrix
cm_VGG16=metrics.confusion_matrix(y_test_vgg16, y_score)

#plotting confusion mattrix
cm_VGG16

# looking at classification report
print(metrics.classification_report(y_test_vgg16, y_score))

#estimating accuracy
VGGaccuracy=metrics.accuracy_score(y_test_vgg16, y_score)
